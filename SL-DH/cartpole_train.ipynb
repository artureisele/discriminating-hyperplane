{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Data 1/2\n",
      "Sampled states\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled inputs\n",
      "Starting next states calculation\n",
      "Sample Data 2/2\n",
      "Sampled states\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled States again\n",
      "Sampled inputs\n",
      "Starting next states calculation\n",
      "Training model...\n",
      "epoch: 0 train loss: 0.01480077570098536\n",
      "epoch: 0 val loss: 0.010322193228548849\n",
      "epoch: 1 train loss: 0.00397049022414296\n",
      "epoch: 1 val loss: 0.005598806827893485\n",
      "epoch: 2 train loss: 0.0024507583835569643\n",
      "epoch: 2 val loss: 0.0018539137154163982\n",
      "epoch: 3 train loss: 0.0006534724763729796\n",
      "epoch: 3 val loss: 0.0015649113135472413\n",
      "epoch: 4 train loss: 0.0006778171169927938\n",
      "epoch: 4 val loss: 0.0004995283778964206\n",
      "epoch: 5 train loss: 0.0003119976989290975\n",
      "epoch: 5 val loss: 0.00032070496797074143\n",
      "epoch: 6 train loss: 0.00020508226444507826\n",
      "epoch: 6 val loss: 0.00017545306581704293\n",
      "epoch: 7 train loss: 0.00015787836174179995\n",
      "epoch: 7 val loss: 0.00011955295124890222\n",
      "epoch: 8 train loss: 0.0001898841557014314\n",
      "epoch: 8 val loss: 8.224235266752512e-05\n",
      "epoch: 9 train loss: 4.7313181810745616e-05\n",
      "epoch: 9 val loss: 8.877652414949114e-05\n",
      "epoch: 10 train loss: 6.96058587806731e-05\n",
      "epoch: 10 val loss: 6.508221715738391e-05\n",
      "epoch: 11 train loss: 3.806536440028416e-05\n",
      "epoch: 11 val loss: 5.8271886264378704e-05\n",
      "epoch: 12 train loss: 3.324587142883524e-05\n",
      "epoch: 12 val loss: 1.9907001607725845e-05\n",
      "epoch: 13 train loss: 2.583643439042596e-05\n",
      "epoch: 13 val loss: 2.0086468174302478e-05\n",
      "epoch: 14 train loss: 2.3260271075964333e-05\n",
      "epoch: 14 val loss: 2.051252867907805e-05\n",
      "epoch: 15 train loss: 1.6056139612168814e-05\n",
      "epoch: 15 val loss: 1.3675693678615817e-05\n",
      "epoch: 16 train loss: 1.0638438311529221e-05\n",
      "epoch: 16 val loss: 1.3333395664783629e-05\n",
      "epoch: 17 train loss: 1.0598351666212827e-05\n",
      "epoch: 17 val loss: 1.3214930481246617e-05\n",
      "epoch: 18 train loss: 1.1431894880898478e-05\n",
      "epoch: 18 val loss: 1.9150403494377187e-05\n",
      "epoch: 19 train loss: 2.7850703839419167e-05\n",
      "epoch: 19 val loss: 3.752292145634505e-05\n",
      "epoch: 20 train loss: 0.00017161450552319587\n",
      "epoch: 20 val loss: 5.5483674120266e-05\n",
      "epoch: 21 train loss: 0.0001373582992829892\n",
      "epoch: 21 val loss: 6.504004756946893e-05\n",
      "epoch: 22 train loss: 7.835509738140068e-05\n",
      "epoch: 22 val loss: 3.5475319594307834e-05\n",
      "epoch: 23 train loss: 3.0490222747443067e-05\n",
      "epoch: 23 val loss: 2.872300951333518e-05\n",
      "epoch: 24 train loss: 2.140280231021751e-05\n",
      "epoch: 24 val loss: 2.1788508555570298e-05\n",
      "epoch: 25 train loss: 1.2286535322799744e-05\n",
      "epoch: 25 val loss: 2.5350937768176622e-05\n",
      "epoch: 26 train loss: 1.1598002969070683e-05\n",
      "epoch: 26 val loss: 1.3322897531669361e-05\n",
      "epoch: 27 train loss: 1.0141396453829658e-05\n",
      "epoch: 27 val loss: 1.1104642813765491e-05\n",
      "epoch: 28 train loss: 8.178028386225942e-06\n",
      "epoch: 28 val loss: 1.4844414666750185e-05\n",
      "epoch: 29 train loss: 1.3777168415140151e-05\n",
      "epoch: 29 val loss: 1.1190321118588656e-05\n",
      "epoch: 30 train loss: 9.516374231787774e-06\n",
      "epoch: 30 val loss: 4.012967409591246e-06\n",
      "epoch: 31 train loss: 1.542501091220938e-05\n",
      "epoch: 31 val loss: 1.0310051192256146e-05\n",
      "epoch: 32 train loss: 1.7929174249927086e-05\n",
      "epoch: 32 val loss: 1.8722423793055337e-05\n",
      "epoch: 33 train loss: 1.1711508198118817e-05\n",
      "epoch: 33 val loss: 0.00010921113497093442\n",
      "epoch: 34 train loss: 0.0001266274045400252\n",
      "epoch: 34 val loss: 0.0003037296577750621\n",
      "epoch: 35 train loss: 0.0001357577939761262\n",
      "epoch: 35 val loss: 0.00012095776491588314\n",
      "epoch: 36 train loss: 0.00017765130604852243\n",
      "epoch: 36 val loss: 6.499818849040471e-05\n",
      "epoch: 37 train loss: 6.961169839218917e-05\n",
      "epoch: 37 val loss: 0.00011991536322325325\n",
      "epoch: 38 train loss: 3.800723786722552e-05\n",
      "epoch: 38 val loss: 0.0001192726246657995\n",
      "epoch: 39 train loss: 1.6880159610202473e-05\n",
      "epoch: 39 val loss: 8.223217103126719e-05\n",
      "epoch: 40 train loss: 4.1342165820572576e-05\n",
      "epoch: 40 val loss: 1.8828145379045973e-05\n",
      "epoch: 41 train loss: 2.6873300640418195e-05\n",
      "epoch: 41 val loss: 2.2470966321552788e-05\n",
      "epoch: 42 train loss: 2.872332440424173e-05\n",
      "epoch: 42 val loss: 2.525789908260836e-05\n",
      "epoch: 43 train loss: 9.926294329830088e-06\n",
      "epoch: 43 val loss: 1.4420779148433754e-05\n",
      "epoch: 44 train loss: 4.299527691742241e-06\n",
      "epoch: 44 val loss: 9.714632513400396e-06\n",
      "epoch: 45 train loss: 3.956141035996489e-06\n",
      "epoch: 45 val loss: 6.405955205145331e-06\n",
      "epoch: 46 train loss: 3.9253353873983055e-06\n",
      "epoch: 46 val loss: 1.7581268709114723e-06\n",
      "epoch: 47 train loss: 3.840070129073982e-06\n",
      "epoch: 47 val loss: 2.5491626457881235e-06\n",
      "epoch: 48 train loss: 2.559374422409701e-06\n",
      "epoch: 48 val loss: 2.6464400023817152e-06\n",
      "epoch: 49 train loss: 3.516258490611075e-06\n",
      "epoch: 49 val loss: 3.0506347272285883e-06\n",
      "epoch: 50 train loss: 5.123850025293789e-06\n",
      "epoch: 50 val loss: 4.551289045717859e-06\n",
      "epoch: 51 train loss: 6.45701226509024e-06\n",
      "epoch: 51 val loss: 6.0513043211257555e-06\n",
      "epoch: 52 train loss: 1.4060733485491368e-06\n",
      "epoch: 52 val loss: 3.821327714333556e-06\n",
      "epoch: 53 train loss: 1.0214537550199471e-06\n",
      "epoch: 53 val loss: 1.8284006109485378e-06\n",
      "epoch: 54 train loss: 1.850745501749177e-06\n",
      "epoch: 54 val loss: 1.7864098172711472e-06\n",
      "epoch: 55 train loss: 1.1480894839418758e-06\n",
      "epoch: 55 val loss: 1.1581885834116722e-06\n",
      "epoch: 56 train loss: 1.1142826877087638e-06\n",
      "epoch: 56 val loss: 8.105118289627243e-07\n",
      "epoch: 57 train loss: 6.038432084036293e-07\n",
      "epoch: 57 val loss: 7.647177717621234e-07\n",
      "epoch: 58 train loss: 7.231136152329054e-07\n",
      "epoch: 58 val loss: 7.389142758164091e-07\n",
      "epoch: 59 train loss: 7.985577357988624e-07\n",
      "epoch: 59 val loss: 6.990002334744134e-07\n",
      "epoch: 60 train loss: 6.835773669868531e-07\n",
      "epoch: 60 val loss: 7.000492850993693e-07\n",
      "epoch: 61 train loss: 3.953612626689678e-07\n",
      "epoch: 61 val loss: 6.694433982155014e-07\n",
      "epoch: 62 train loss: 7.839258733692439e-07\n",
      "epoch: 62 val loss: 7.639168489169073e-07\n",
      "epoch: 63 train loss: 1.1153459676142864e-06\n",
      "epoch: 63 val loss: 1.2849247427486932e-06\n",
      "epoch: 64 train loss: 2.0352481598760518e-06\n",
      "epoch: 64 val loss: 1.2101899726754214e-06\n",
      "epoch: 65 train loss: 1.4522999678503783e-06\n",
      "epoch: 65 val loss: 2.23692750844642e-06\n",
      "epoch: 66 train loss: 2.882116894681635e-06\n",
      "epoch: 66 val loss: 7.210853782613333e-06\n",
      "epoch: 67 train loss: 2.848559373227915e-05\n",
      "epoch: 67 val loss: 1.9436254941880637e-05\n",
      "epoch: 68 train loss: 4.227944545191473e-05\n",
      "epoch: 68 val loss: 4.555625723617787e-05\n",
      "epoch: 69 train loss: 0.00012192780805050787\n",
      "epoch: 69 val loss: 0.00014695032246793982\n",
      "epoch: 70 train loss: 0.0005437280155320855\n",
      "epoch: 70 val loss: 0.00016311684154566124\n",
      "epoch: 71 train loss: 0.0001948897319034224\n",
      "epoch: 71 val loss: 9.055900128874684e-05\n",
      "epoch: 72 train loss: 0.00014491836983263987\n",
      "epoch: 72 val loss: 3.655508374700807e-05\n",
      "epoch: 73 train loss: 0.00018142679618289833\n",
      "epoch: 73 val loss: 0.00012252622661281423\n",
      "epoch: 74 train loss: 4.5631385429038035e-05\n",
      "epoch: 74 val loss: 0.00015216994033531722\n",
      "epoch: 75 train loss: 2.9604417419166872e-05\n",
      "epoch: 75 val loss: 9.737080871768804e-05\n",
      "epoch: 76 train loss: 8.54552568674321e-05\n",
      "epoch: 76 val loss: 2.1502376939106493e-05\n",
      "epoch: 77 train loss: 1.9031084820911387e-05\n",
      "epoch: 77 val loss: 3.4902407107211514e-05\n",
      "epoch: 78 train loss: 6.247141198933253e-06\n",
      "epoch: 78 val loss: 2.653014461736338e-05\n",
      "epoch: 79 train loss: 1.1629340012436434e-05\n",
      "epoch: 79 val loss: 7.473465596205065e-06\n",
      "epoch: 80 train loss: 1.039408316645535e-05\n",
      "epoch: 80 val loss: 1.0088440496844595e-05\n",
      "epoch: 81 train loss: 6.2311439520177925e-06\n",
      "epoch: 81 val loss: 5.787781179028572e-06\n",
      "epoch: 82 train loss: 3.862229672936819e-06\n",
      "epoch: 82 val loss: 2.156989605468072e-06\n",
      "epoch: 83 train loss: 3.0235713121846513e-06\n",
      "epoch: 83 val loss: 2.5534201026946575e-06\n",
      "epoch: 84 train loss: 2.722120558998288e-06\n",
      "epoch: 84 val loss: 1.2677837911477182e-06\n",
      "epoch: 85 train loss: 1.0846532365626547e-06\n",
      "epoch: 85 val loss: 1.8338731725483465e-06\n",
      "epoch: 86 train loss: 1.0990892349494497e-06\n",
      "epoch: 86 val loss: 1.664457158444394e-06\n",
      "epoch: 87 train loss: 9.584797097793686e-07\n",
      "epoch: 87 val loss: 9.43283111226443e-07\n",
      "epoch: 88 train loss: 8.837064942066696e-07\n",
      "epoch: 88 val loss: 7.948685833278915e-07\n",
      "epoch: 89 train loss: 7.487816170543556e-07\n",
      "epoch: 89 val loss: 6.946473772993886e-07\n",
      "epoch: 90 train loss: 6.469755135905832e-07\n",
      "epoch: 90 val loss: 7.269847256759218e-07\n",
      "epoch: 91 train loss: 5.321330621573141e-07\n",
      "epoch: 91 val loss: 5.897933563915511e-07\n",
      "epoch: 92 train loss: 4.4324667070187e-07\n",
      "epoch: 92 val loss: 5.891329120781523e-07\n",
      "epoch: 93 train loss: 4.1459047422791797e-07\n",
      "epoch: 93 val loss: 5.125016240474656e-07\n",
      "epoch: 94 train loss: 3.828523476158103e-07\n",
      "epoch: 94 val loss: 4.865427229213645e-07\n",
      "epoch: 95 train loss: 3.6204011099766215e-07\n",
      "epoch: 95 val loss: 4.495718291218356e-07\n",
      "epoch: 96 train loss: 3.4079264102021907e-07\n",
      "epoch: 96 val loss: 4.3252367928837646e-07\n",
      "epoch: 97 train loss: 3.2346662761794465e-07\n",
      "epoch: 97 val loss: 4.118933009422086e-07\n",
      "epoch: 98 train loss: 3.0824878778552683e-07\n",
      "epoch: 98 val loss: 3.9551291161626316e-07\n",
      "epoch: 99 train loss: 2.9317645543929526e-07\n",
      "epoch: 99 val loss: 3.8258111518812813e-07\n",
      "epoch: 100 train loss: 2.804088380719041e-07\n",
      "epoch: 100 val loss: 3.6613587709659256e-07\n",
      "epoch: 101 train loss: 2.6865797954206593e-07\n",
      "epoch: 101 val loss: 3.548896665478708e-07\n",
      "epoch: 102 train loss: 2.578292507011894e-07\n",
      "epoch: 102 val loss: 3.4346134139024187e-07\n",
      "epoch: 103 train loss: 2.4793838595678814e-07\n",
      "epoch: 103 val loss: 3.325093936637695e-07\n",
      "epoch: 104 train loss: 2.38869255418918e-07\n",
      "epoch: 104 val loss: 3.2433635499421854e-07\n",
      "epoch: 105 train loss: 2.3041898133188956e-07\n",
      "epoch: 105 val loss: 3.145422165840441e-07\n",
      "epoch: 106 train loss: 2.2260282747167933e-07\n",
      "epoch: 106 val loss: 3.073215135729036e-07\n",
      "epoch: 107 train loss: 2.1524841273829664e-07\n",
      "epoch: 107 val loss: 2.9893725789166254e-07\n",
      "epoch: 108 train loss: 2.084077127690557e-07\n",
      "epoch: 108 val loss: 2.9231476367173267e-07\n",
      "epoch: 109 train loss: 2.020676473482591e-07\n",
      "epoch: 109 val loss: 2.854291102967607e-07\n",
      "epoch: 110 train loss: 1.9612263933871278e-07\n",
      "epoch: 110 val loss: 2.7962762076971723e-07\n",
      "epoch: 111 train loss: 1.9050096145005953e-07\n",
      "epoch: 111 val loss: 2.738553695551705e-07\n",
      "epoch: 112 train loss: 1.8526292520911584e-07\n",
      "epoch: 112 val loss: 2.683429167297407e-07\n",
      "epoch: 113 train loss: 1.803918286141803e-07\n",
      "epoch: 113 val loss: 2.6332090522076024e-07\n",
      "epoch: 114 train loss: 1.758451368253425e-07\n",
      "epoch: 114 val loss: 2.5826353165784113e-07\n",
      "epoch: 115 train loss: 1.7151415192056396e-07\n",
      "epoch: 115 val loss: 2.5350553112306085e-07\n",
      "epoch: 116 train loss: 1.67430613144153e-07\n",
      "epoch: 116 val loss: 2.491080424846038e-07\n",
      "epoch: 117 train loss: 1.6349078089863797e-07\n",
      "epoch: 117 val loss: 2.4521540320906517e-07\n",
      "epoch: 118 train loss: 1.5973393578696655e-07\n",
      "epoch: 118 val loss: 2.4069482343489535e-07\n",
      "epoch: 119 train loss: 1.5615709358556156e-07\n",
      "epoch: 119 val loss: 2.366262937702624e-07\n",
      "epoch: 120 train loss: 1.5274784107180152e-07\n",
      "epoch: 120 val loss: 2.3318820939513468e-07\n",
      "epoch: 121 train loss: 1.49569338705371e-07\n",
      "epoch: 121 val loss: 2.2938477942983376e-07\n",
      "epoch: 122 train loss: 1.465062309234069e-07\n",
      "epoch: 122 val loss: 2.2582487052311575e-07\n",
      "epoch: 123 train loss: 1.4352139972475302e-07\n",
      "epoch: 123 val loss: 2.2298976820245548e-07\n",
      "epoch: 124 train loss: 1.407006328208239e-07\n",
      "epoch: 124 val loss: 2.1988034955702345e-07\n",
      "epoch: 125 train loss: 1.3797543913972933e-07\n",
      "epoch: 125 val loss: 2.171367550670151e-07\n",
      "epoch: 126 train loss: 1.3541085541304286e-07\n",
      "epoch: 126 val loss: 2.1414884724983165e-07\n",
      "epoch: 127 train loss: 1.3286760497155988e-07\n",
      "epoch: 127 val loss: 2.118253280327007e-07\n",
      "epoch: 128 train loss: 1.3050722161233788e-07\n",
      "epoch: 128 val loss: 2.0884674396125114e-07\n",
      "epoch: 129 train loss: 1.2811438254733952e-07\n",
      "epoch: 129 val loss: 2.0655261699277912e-07\n",
      "epoch: 130 train loss: 1.2607178238844343e-07\n",
      "epoch: 130 val loss: 2.0515924903942453e-07\n",
      "epoch: 131 train loss: 1.236818244616418e-07\n",
      "epoch: 131 val loss: 2.0155225885569583e-07\n",
      "epoch: 132 train loss: 1.2160963162804288e-07\n",
      "epoch: 132 val loss: 1.998105859513048e-07\n",
      "epoch: 133 train loss: 1.1975121546002906e-07\n",
      "epoch: 133 val loss: 1.9648713980135413e-07\n",
      "epoch: 134 train loss: 1.1763509088861595e-07\n",
      "epoch: 134 val loss: 1.947050955614234e-07\n",
      "epoch: 135 train loss: 1.1572156116970247e-07\n",
      "epoch: 135 val loss: 1.926573544803762e-07\n",
      "epoch: 136 train loss: 1.1387931526559044e-07\n",
      "epoch: 136 val loss: 1.9110647383873603e-07\n",
      "epoch: 137 train loss: 1.1219431857662215e-07\n",
      "epoch: 137 val loss: 1.9039630599737666e-07\n",
      "epoch: 138 train loss: 1.1029780421694931e-07\n",
      "epoch: 138 val loss: 1.8724344084359713e-07\n",
      "epoch: 139 train loss: 1.0866684466156584e-07\n",
      "epoch: 139 val loss: 1.8489559558367574e-07\n",
      "epoch: 140 train loss: 1.0705081813587286e-07\n",
      "epoch: 140 val loss: 1.84319670066097e-07\n",
      "epoch: 141 train loss: 1.0573517939788068e-07\n",
      "epoch: 141 val loss: 1.8392050439872662e-07\n",
      "epoch: 142 train loss: 1.0381969579250695e-07\n",
      "epoch: 142 val loss: 1.8160411916578507e-07\n",
      "epoch: 143 train loss: 1.0238243992507146e-07\n",
      "epoch: 143 val loss: 1.7903864709225918e-07\n",
      "epoch: 144 train loss: 1.0102656687638286e-07\n",
      "epoch: 144 val loss: 1.765101148053185e-07\n",
      "epoch: 145 train loss: 9.94106040774991e-08\n",
      "epoch: 145 val loss: 1.7549918324478668e-07\n",
      "epoch: 146 train loss: 9.833386739995955e-08\n",
      "epoch: 146 val loss: 1.7597362821327206e-07\n",
      "epoch: 147 train loss: 9.683452240517427e-08\n",
      "epoch: 147 val loss: 1.7184052487340037e-07\n",
      "epoch: 148 train loss: 9.547283080321402e-08\n",
      "epoch: 148 val loss: 1.703396767426771e-07\n",
      "epoch: 149 train loss: 9.419091801030306e-08\n",
      "epoch: 149 val loss: 1.7038344147860647e-07\n",
      "epoch: 150 train loss: 9.323423446504474e-08\n",
      "epoch: 150 val loss: 1.6671148610336992e-07\n",
      "epoch: 151 train loss: 9.276738071532467e-08\n",
      "epoch: 151 val loss: 1.6959003172186996e-07\n",
      "epoch: 152 train loss: 9.067352253431755e-08\n",
      "epoch: 152 val loss: 1.6688725244125746e-07\n",
      "epoch: 153 train loss: 8.974224963981423e-08\n",
      "epoch: 153 val loss: 1.6363957791691134e-07\n",
      "epoch: 154 train loss: 8.84291042755276e-08\n",
      "epoch: 154 val loss: 1.622294137566218e-07\n",
      "epoch: 155 train loss: 8.791662664563969e-08\n",
      "epoch: 155 val loss: 1.6481736063894183e-07\n",
      "epoch: 156 train loss: 8.678351314852185e-08\n",
      "epoch: 156 val loss: 1.589762465424317e-07\n",
      "epoch: 157 train loss: 8.53430804358541e-08\n",
      "epoch: 157 val loss: 1.6023176860393585e-07\n",
      "epoch: 158 train loss: 8.478402478349549e-08\n",
      "epoch: 158 val loss: 1.5627163955521943e-07\n",
      "epoch: 159 train loss: 8.506840821920676e-08\n",
      "epoch: 159 val loss: 1.611526601993518e-07\n",
      "epoch: 160 train loss: 8.241660289626833e-08\n",
      "epoch: 160 val loss: 1.5730666203089845e-07\n",
      "epoch: 161 train loss: 8.211601162556465e-08\n",
      "epoch: 161 val loss: 1.5311833067507807e-07\n",
      "epoch: 162 train loss: 8.165746928832187e-08\n",
      "epoch: 162 val loss: 1.5765918743424336e-07\n",
      "epoch: 163 train loss: 8.075388267372185e-08\n",
      "epoch: 163 val loss: 1.5077178884949148e-07\n",
      "epoch: 164 train loss: 7.899302109531051e-08\n",
      "epoch: 164 val loss: 1.527311490415979e-07\n",
      "epoch: 165 train loss: 7.826820407617504e-08\n",
      "epoch: 165 val loss: 1.5304739961207524e-07\n",
      "epoch: 166 train loss: 7.911531752958574e-08\n",
      "epoch: 166 val loss: 1.4744075980725234e-07\n",
      "epoch: 167 train loss: 7.752114150486771e-08\n",
      "epoch: 167 val loss: 1.52596820420088e-07\n",
      "epoch: 168 train loss: 7.688349375036752e-08\n",
      "epoch: 168 val loss: 1.458239989533775e-07\n",
      "epoch: 169 train loss: 7.529710022028805e-08\n",
      "epoch: 169 val loss: 1.4974348612101026e-07\n",
      "epoch: 170 train loss: 7.409965913399414e-08\n",
      "epoch: 170 val loss: 1.4604055212932011e-07\n",
      "epoch: 171 train loss: 7.336931817079979e-08\n",
      "epoch: 171 val loss: 1.441434932356275e-07\n",
      "epoch: 172 train loss: 7.97555550150297e-08\n",
      "epoch: 172 val loss: 1.52968132929253e-07\n",
      "epoch: 173 train loss: 7.194579364061437e-08\n",
      "epoch: 173 val loss: 1.4681516955316912e-07\n",
      "epoch: 174 train loss: 7.479127247670765e-08\n",
      "epoch: 174 val loss: 1.4088961043041288e-07\n",
      "epoch: 175 train loss: 7.133389659030217e-08\n",
      "epoch: 175 val loss: 1.3928090063183697e-07\n",
      "epoch: 176 train loss: 7.398348456799157e-08\n",
      "epoch: 176 val loss: 1.4670633816489828e-07\n",
      "epoch: 177 train loss: 7.24830853663911e-08\n",
      "epoch: 177 val loss: 1.4649002176816622e-07\n",
      "epoch: 178 train loss: 7.040602245452082e-08\n",
      "epoch: 178 val loss: 1.3846681323878259e-07\n",
      "epoch: 179 train loss: 7.812345596488351e-08\n",
      "epoch: 179 val loss: 1.5101691487608008e-07\n",
      "epoch: 180 train loss: 6.781397600802482e-08\n",
      "epoch: 180 val loss: 1.41627810439483e-07\n",
      "epoch: 181 train loss: 6.992353965224099e-08\n",
      "epoch: 181 val loss: 1.3901105628539025e-07\n",
      "epoch: 182 train loss: 6.831273287385232e-08\n",
      "epoch: 182 val loss: 1.3742260382545077e-07\n",
      "epoch: 183 train loss: 7.14469141865892e-08\n",
      "epoch: 183 val loss: 1.4844120119139145e-07\n",
      "epoch: 184 train loss: 7.03408952929346e-08\n",
      "epoch: 184 val loss: 1.338725092547191e-07\n",
      "epoch: 185 train loss: 7.286399524287406e-08\n",
      "epoch: 185 val loss: 1.4562903668882264e-07\n",
      "epoch: 186 train loss: 6.649734609707043e-08\n",
      "epoch: 186 val loss: 1.3270110482792857e-07\n",
      "epoch: 187 train loss: 6.816683194453081e-08\n",
      "epoch: 187 val loss: 1.4479780588211698e-07\n",
      "epoch: 188 train loss: 6.96119580714855e-08\n",
      "epoch: 188 val loss: 1.30279439585444e-07\n",
      "epoch: 189 train loss: 7.182874674688414e-08\n",
      "epoch: 189 val loss: 1.5029127942748174e-07\n",
      "epoch: 190 train loss: 7.79549077788711e-08\n",
      "epoch: 190 val loss: 1.3414308366682205e-07\n",
      "epoch: 191 train loss: 8.438620062862534e-08\n",
      "epoch: 191 val loss: 1.6046534428724256e-07\n",
      "epoch: 192 train loss: 6.801412432886229e-08\n",
      "epoch: 192 val loss: 1.4117862107823077e-07\n",
      "epoch: 193 train loss: 1.0193273453822155e-07\n",
      "epoch: 193 val loss: 1.5998575104743505e-07\n",
      "epoch: 194 train loss: 1.408465018257757e-07\n",
      "epoch: 194 val loss: 1.782118527393554e-07\n",
      "epoch: 195 train loss: 1.2601175376788773e-07\n",
      "epoch: 195 val loss: 1.4141103720905832e-07\n",
      "epoch: 196 train loss: 1.3623223574995736e-07\n",
      "epoch: 196 val loss: 1.3960132435310463e-07\n",
      "epoch: 197 train loss: 1.9726694890061596e-06\n",
      "epoch: 197 val loss: 1.4247845213809579e-05\n",
      "epoch: 198 train loss: 4.403581143930007e-05\n",
      "epoch: 198 val loss: 0.0006559635042994503\n",
      "epoch: 199 train loss: 0.0015014514898388227\n",
      "epoch: 199 val loss: 0.0004430138936929861\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=4, out_features=1000, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (5): ReLU()\n",
       "  (6): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (7): ReLU()\n",
       "  (8): Linear(in_features=1000, out_features=1000, bias=True)\n",
       "  (9): ReLU()\n",
       "  (10): Linear(in_features=1000, out_features=2, bias=True)\n",
       "  (11): Identity()\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from envs.cartpole import Cartpole\n",
    "from controllers.constant import target\n",
    "from controllers.QP import QP_CBF_controller\n",
    "from filters.disc import disc\n",
    "from inv_set.cartpole import trap\n",
    "from filters.NN_filters import FCN\n",
    "from inv_set.inv_ped import cbf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as th\n",
    "\n",
    "# initialize cartpole\n",
    "inv_set = trap(0.5, -5)\n",
    "disc_steps = 2\n",
    "env = Cartpole(inv_set, disc_steps)\n",
    "\n",
    "# initialize network\n",
    "input_size = 4\n",
    "output_size = 2\n",
    "n_layers = 5\n",
    "size=1000\n",
    "activation = 'relu'\n",
    "lr = 5e-4\n",
    "sc = 1\n",
    "gamma_pos = 1\n",
    "gamma_neg = 5\n",
    "model = FCN(input_size, output_size, n_layers, size, activation, lr, sc, gamma_pos, gamma_neg)\n",
    "model.FCN.train()\n",
    "\n",
    "# train network\n",
    "num_states = 10000\n",
    "num_inputs = 300\n",
    "epochs = 200\n",
    "num_log = 4\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "best_loss = float('inf')\n",
    "print(\"Sample Data 1/2\")\n",
    "val_states, val_inputs, val_labels = env.sample_data(num_states, num_inputs)\n",
    "print(\"Sample Data 2/2\")\n",
    "train_states, train_inputs, train_labels = env.sample_data(num_states, num_inputs)\n",
    "plt.scatter(val_states[:, 0], val_states[:, 1])\n",
    "plt.savefig('exps/cartpole/states.png')\n",
    "plt.clf()\n",
    "print(\"Training model...\")\n",
    "for epoch in range(epochs):\n",
    "    for i in range(5):\n",
    "        train_loss = model.update2(train_states, train_inputs, train_labels, env)\n",
    "    model.scheduler.step()\n",
    "    if epoch % 1 == 0:\n",
    "        train_losses.append(train_loss)\n",
    "        val_loss = model.get_val_loss(val_states, val_inputs, val_labels, env)\n",
    "        val_losses.append(val_loss)\n",
    "        if val_loss < best_loss:\n",
    "            th.save({\n",
    "            'model_state_dict': model.FCN.state_dict(),\n",
    "            'optimizer_state_dict': model.optimizer.state_dict(),\n",
    "            }, 'exps/cartpole/model.pth')\n",
    "        train_losses = train_losses[-500:]\n",
    "        val_losses = val_losses[-500:]\n",
    "        print(\"epoch:\", epoch, 'train loss:', train_loss)\n",
    "        print(\"epoch:\", epoch, 'val loss:', val_loss)\n",
    "\n",
    "model.FCN.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Running exps...\n",
      "Starting next states calculation\n",
      "Starting next states calculation\n",
      "Starting next states calculation\n",
      "Starting next states calculation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n",
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/sklearn/svm/_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting next states calculation\n",
      "Starting next states calculation\n",
      "Starting next states calculation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "# Set autoreload to update modules automatically before executing code\n",
    "%autoreload 2\n",
    "from envs.cartpole import Cartpole\n",
    "from controllers.constant import target\n",
    "from controllers.QP import QP_CBF_controller\n",
    "from filters.disc import disc\n",
    "from inv_set.cartpole import trap\n",
    "from filters.NN_filters import FCN\n",
    "from inv_set.inv_ped import cbf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "inv_set = trap(0.5, -5)\n",
    "disc_steps = 2\n",
    "env = Cartpole(inv_set, disc_steps)\n",
    "input_size = 4\n",
    "output_size = 2\n",
    "n_layers = 5\n",
    "size=1000\n",
    "activation = 'relu'\n",
    "lr = 5e-4\n",
    "sc = 1\n",
    "gamma_pos = 1\n",
    "gamma_neg = 5\n",
    "nn_filter = FCN(input_size, output_size, n_layers, size, activation, lr, sc, gamma_pos, gamma_neg)\n",
    "checkpoint = torch.load('exps/cartpole/model.pth', map_location=torch.device('cpu'))\n",
    "nn_filter.FCN.load_state_dict(checkpoint['model_state_dict'])\n",
    "nn_filter.FCN.eval()\n",
    "\n",
    "class bang():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(x, t):\n",
    "        if t <= 10:\n",
    "            return 1\n",
    "        elif t <= 20:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    \n",
    "  \n",
    "target_controller = bang()\n",
    "\n",
    "disc_filter = disc(env, 1000)\n",
    "disc_controller = QP_CBF_controller(bang, disc_filter, 1)\n",
    "\n",
    "nn_controller = QP_CBF_controller(bang, nn_filter, 1)\n",
    "\n",
    "# run system and log results\n",
    "print(\"Running exps...\")\n",
    "state = env.reset()\n",
    "t = 300\n",
    "traj_disc = env.run_system(state, t, disc_controller)\n",
    "traj_nn = env.run_system(state, t, nn_controller)\n",
    "plt.scatter(state[0], state[1], c='r')\n",
    "\n",
    "plt.xlim(-1, 1)\n",
    "plt.ylim(-6, 6)\n",
    "plt.plot(traj_disc[:, 0], traj_disc[:, 1], label='disc')\n",
    "plt.plot(traj_nn[:, 0], traj_nn[:, 1], label='nn')\n",
    "plt.legend()\n",
    "plt.savefig('exps/cartpole/traj.png')\n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/artur/miniconda3/envs/fsrl/lib/python3.8/site-packages/gym/spaces/box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.0101]], grad_fn=<SliceBackward0>),\n",
       " tensor([-0.0027], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from envs.cartpole import Cartpole\n",
    "from controllers.constant import target\n",
    "from controllers.QP import QP_CBF_controller\n",
    "from filters.disc import disc\n",
    "from inv_set.cartpole import trap\n",
    "from filters.NN_filters import FCN\n",
    "from inv_set.inv_ped import cbf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch \n",
    "inv_set = trap(0.5, -5)\n",
    "disc_steps = 2\n",
    "env = Cartpole(inv_set, disc_steps)\n",
    "input_size = 4\n",
    "output_size = 2\n",
    "n_layers = 5\n",
    "size=1000\n",
    "activation = 'relu'\n",
    "lr = 5e-4\n",
    "sc = 1\n",
    "gamma_pos = 1\n",
    "gamma_neg = 5\n",
    "nn_filter = FCN(input_size, output_size, n_layers, size, activation, lr, sc, gamma_pos, gamma_neg)\n",
    "checkpoint = torch.load('exps/cartpole/model.pth', map_location=torch.device('cpu'))\n",
    "nn_filter.FCN.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.FCN.eval()\n",
    "\n",
    "model.forward([0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"exps/cartpole/training_data.npz\", train_states = train_states, train_inputs = train_inputs, train_labels = train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\"exps/cartpole/validation_data.npz\", val_states = val_states, val_inputs = val_inputs, val_labels = val_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fsrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
